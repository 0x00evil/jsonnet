{% include 'header.html.jinja' %}

{% set srcroot = 'https://github.com/google/jsonnet/blob/master/case_studies' %}

<h1>Cloud Application Case Study</h1>

<!-- needs index here -->

<p> This case study illustrates that <a href="index.html">Jsonnet</a> can be used to centralize,
unify, and manage configuration for all parts of a realistic web application hosted on the cloud.
That includes application configuration files for each piece of software involved, database schemas
and initial data sets, system configuration files, package manifests, software build configurations,
image configurations (<a href="http://www.packer.io/">Packer</a>) and cloud resources / connectivity
(<a href="http://www.terraform.io/">Terraform</a>).  Although the running example is deployed on <a
href="https://cloud.google.com">Google Cloud Platform</a> and uses specific application software,
Jsonnet can be used to generate configuration for any application and (via Packer &amp; Terraform) a
wide range of cloud providers.  It is assumed that the reader has read the Jsonnet <a
href="tutorial.html">tutorial</a>, and has a basic knowledge of Packer and Terraform.</p>


<h2>Example Web Application</h2>

<a href="fractal_screenshot.png"><img src="fractal_screenshot.png" class=thumb></a>

<p>The example application allows the user to explore (zoom and pan) a Mandelbrot fractal, rendered
server side by some C++ code.  The application is provisionally hosted <a
href="http://fractal.noip.me/">here</a>, but you can easily deploy your own.  The user is able to
save the location of interesting artifacts they find in the fractal, and a time-ordered list of
these with thumbnails is displayed on the left hand side.</p>

<p>Although admittedly a little contrived, this example is intended to represent the structure of a
typical non-trivial real-world web application, and all of its components are available in the
Jsonnet Github repository.  It consists of 3 tiers (illustrated below):  An application server, a
backend database (Cassandra) and a backend service for generating the fractal PNG tiles (C++).  Each
tier is scalable and fault-tolerant.</p>

<img src="fractal_architecture.png">

<p>The application server hosts static content (CSS) and Jinja'd HTML.  The HTML <a
href="{{srcroot}}/fractal/appserv/templates/page.html">contains</a> Javascript that issues AJAX
requests to 1) the tile generation backend and 2) the application server (to fetch / amend the
discoveries list).  The application server therefore does not communicate directly with the fractal
tile generation service, but it needs to know the host:port endpoint in order to embed it in the
HTML so that the user's browser can do so.  The user does not communicate directly with the
Cassandra database.</p>

<p>Both the application server and the tile generation service use Nginx, uWSGI and flask to host
their content.  For the application server, this means transforming HTTP requests into database
accesses and/or serving content (code <a href="{{srcroot}}/appserv/main.py">here</a>).  For the
tile generation service, this means invoking a compiled C++ <a
href="{{srcroot}}/tilegen/mandelbrot.cpp">executable</a> from the Flask <a
href="{{srcroot}}/tilegen/mandelbrot_service.py">handler</a> in order to construct a PNG for a given
tile / thumbnail of the fractal.  Both tiers consist of a group of machines behind a layer 3 cloud
load balancer with a static IP and a simple health check.  The Cassandra database is simply a set of
instances, as the Cassandra client library (used by the application server) does its own load
balancing and transparent failover.</p>

<p>The application is deployed by first building a image for each of the 3 kinds of cloud
instances (application server, fractal image processing service, Cassandra) using Packer.  Then, all
the cloud resources are deployed using Terraform.  The Packer build compiles / installs and
configures all of the required software on each image.  The Terraform configuration uses instance
metadata to communicate last minute configuration info (connectivity, passwords) that we cannot or
prefer not to embed in the images.  The choice about what to do at image build vs deployment
time is flexible.  In our case, we try to do all time consuming steps at build time, allowing new
instances to be brought up very quickly.  This would be particularly important in an autoscaling
situation.</p>


<h2>Configuration Structure</h2>

<p>A single Jsonnet configuration yields the 3 Packer builds (*.packer.json) and the Terraform
configuration (terraform.tf), using <a href="commandline.html#multi">multiple file output</a>.
Those files in turn embed other configurations for the application software that will run on the
instances.</p>

<p>The Jsonnet configuration is divided into several files (via the import construct), to promote
re-use of some abstracted parts of the configuration, as well as having a separate credentials file
(to avoid checking passwords into version control).  Note that Jsonnet did not require this
structure:  Other separations (or no separation) would also have been possible.  Note also that the
Jsonnet template libraries include some things not used in this example application, e.g. PostgreSQL
and MySQL templates.</p>

{% set service_jsonnet =
        '<tt><a href="' + srcroot + '/fractal/service.jsonnet">service.jsonnet</a></tt>' %}

{% set packer_jsonnet =
        '<tt><a href="' + srcroot + '/lib/packer.jsonnet">lib/packer.jsonnet</a></tt>' %}

{% set terraform_jsonnet =
        '<tt><a href="' + srcroot + '/lib/terraform.jsonnet">lib/terraform.jsonnet</a></tt>' %}

{% set cassandra_jsonnet =
        '<tt><a href="' + srcroot + '/lib/cassandra.jsonnet">lib/cassandra.jsonnet</a></tt>' %}

{% set makefile = '<tt><a href="' + srcroot + '/fractal/Makefile">Makefile</a></tt>' %}

<ul>

<li>{{service_jsonnet|safe}}: The Fractal application configuration, as a list of packer
images and a Terraform configuration.  Note the first few lines import the other Jsonnet files and
store their contents in top-level scoped variables of the same names.  When we access things from
those libraries we will do so through the top-level variables.</li>

<li><tt>credentials.jsonnet: User and superuser keys for Cassandra, and Google Cloud Platform
project name.  A template for this file is <a
href="{{srcroot}}/fractal/credentials.jsonnet.TEMPLATE">provided</a>.</li>

<li>{{packer_jsonnet|safe}}: Some templates to help with writing Packer configurations.</li>

<li>{{terraform_jsonnet|safe}}: Some templates to help with writing Terraform
configurations.</li>

<li>{{cassandra_jsonnet|safe}}: Some templates to help with writing Packer and Terraform
configurations for Cassandra.</li>

</ul>

<p>To integrate Jsonnet with Packer and Terraform, a {{makefile|safe}} is used.  This first runs
Jsonnet on the configuration and then runs Packer / Terraform on the resulting files (if they have
changed).  Using make as an ops tool is unusual, and probably not advisable in production.  However
it is well understood, available everywhere, and does 3 things that we want:  Invoke other programs,
run things in parallel, and avoid repeating work that is already complete.  For example, Packer is
only invoked if its configuration file has changed, and the 3 image builds will proceed in parallel
(they take a few minutes each).  The choice of 'glue' is not particularly relevant to this case
study.  One could also use a small Python script.</p>

<p>Ignoring the re-usable templates, whitespace, and comments, the Jsonnet configuration is 217
lines (9.7kB).  The generated Packer and Terraform files are 740 lines (25kB) in total.  This
demonstrates the productivity benefits of using template expansion when writing configurations.</p>


<h3>Whirlwind Tour</h3>

<p> The fractal example is a complete application and therefore its configuration is quite long and
technical.  Some interesting features are described below, but to gain a complete understanding of
this configuration would require studying the full Jsonnet files.  If parts are unclear, please seek
answers from the <a href="https://groups.google.com/forum/#!forum/jsonnet">mailing list</a>.</p>


<h4>Packer And Application Configuration</h4>

<p>The <code>ImageMixin</code> object in {{service_jsonnet|safe}} is used to factor out common
configuration from the 3 images (Packer configurations).  This includes the Google Cloud Platform
project id and filename of the service account key.  Since all the images are derived ultimately
from the <code>GcpDebian</code> image (in {{packer_jsonnet|safe}}), and this image includes apt
&amp; pip provisioners (discussed shortly), this is also a good place to ensure some basic packages
are installed on every image.  Note that since this object is not intended to be actually created
(it existing only for reference from other parts of the configuration) it is stored in a hidden
field (:: syntax).</p>

<p>Both the application server's image configuration <tt>appserv.packer.json</tt> and the tile
generation service's image configuration <tt>tilegen.packer.json</tt> extend
<code>MyFlaskImage</code>, which exists merely to add the aforementioned <code>ImageMixin</code> to
the <code>GcpDebianNginxUwsgiFlaskImage</code> template from {{packer_jsonnet|safe}}.  That template
builds on the basic <code>GcpDebianImage</code> template from the same library, and adds all the
packages (and default configuration) for both Nginx and uWSGI.</p>

<p>In Jsonnet we use JSON as the canonical data model and convert to other formats as needed.  An
example of this is the uWSGI configuration, an <a
href="http://en.wikipedia.org/wiki/INI_file">INI</a> file, which specified in Jsonnet under the
<code>uwsgiConf</code> field in <code>GcpDebianNginxUwsgiFlaskImage</code>.  The JSON version is
converted to INI by the call to <code>std.manifestIni</code> (documented <a
href="stdlib.html">here</a>) in the provisioner below.  Representing the INI file with the JSON
object model (instead of as a string) allows elements of the uWSGI configuration, such as the
filename of the UNIX domain socket, to be easily kept in sync with other elements of the
configuration, e.g. the Nginx configuration file below needs the same filename.  If the application
is configured with JSON, or even YAML, then no conversion is required.  An example of that is the
default Cassandra configuration file held in the <code>conf</code> field of
{{cassandra_jsonnet|safe}}.</p>

<p>Looking back at <tt>appserv.packer.json</tt> and <tt>tilegen.packer.json</tt> in
{{service_jsonnet|safe}}, while both use Nginx/uWSGI/Flask there are some subtle differences.  Since
the HTTP request handlers are different in each case, the module (uWSGI entrypoint) and also
required packages are different.  Firstly, the tile generation image needs a provisioner to build
the C++ code.  Secondly, since the application server talks to the Cassandra database using
cassandra-driver, which interferes with preforking, it was necessary to override the
<code>lazy</code> field of the uWSGI configuration in that image.  This is an example of how an
abstract template can unify two similar parts of the configuration, while still allowing the
overriding of small details as needed.  Note also that such precise manipulation of configuration
details would be much harder if the uWSGI configuration was represented as a single string instead
of as a structure within the object model.</p>

<p>Going up to the top of the template hierarchy we have <code>GcpDebianImage</code> and finally
<code>GcpImage</code> in {{packer_jsonnet|safe}}.  The latter gives the Packer builder configuration
for Google Cloud Platform, bringing out some fields to the top level, essentially hiding the
<code>builder</code> subobject.  We can hide the builder configuration because we only ever need one
builder per image.  We can support multiple cloud providers by deriving an entire new Packer
configuration at the top level, overriding as necessary to specialize for that platform.  The
<code>GcpDebianImage</code> selects the base image (Backports) and adds provisioners for apt and pip
packages.  The configuration of those provisioners (the list of installed packages, and additional
repositories / keys) is brought out to the top level of the image configuration.  By default, the
lists are empty but subobjects can override and extend them as we saw with
<code>GcpDebianNginxUwsgiFlaskImage</code>.</p>

<p>The actual provisioners <code>Apt</code> and <code>Pip</code> are defined further up
{{packer_jsonnet|safe}}.  In their definitions, one can see how the various shell commands are built
up from the declarative lists of repositories, package names, etc.  Installing packages in a
non-interactive context requires a few extra switches and an environment variable, but the
<code>Apt</code> provisioner handles all that.  Also note how these provisioners both derive from
<code>RootShell</code> (defined right at the top of the file) because those commands need to be run
as root.  All of this is plain Jsonnet code, so it is possible to create your own provisioners
(based on these or from scratch) in order to declaratively control packages in a Packer image.</p>


<h4>Terraform</h4>

<p>The remainder of {{service_jsonnet|safe}} is the Terraform configuration that defines all the
cloud resources that are required to run the fractal web application.  That includes the instances
of the images built by Packer, but also resources that configure network routing / firewalls, load
balancers, etc.</p>

<p>The function <code>zone</code> is used to statically assign zones to instances on a round robin
basis.  All the instances extend from <code>FractalInstance</code>, which is parameterized by the
index <code>zone_hash</code> (it's actually just a function that takes zone_hash and returns an
instance template).  It is this index that is used to compute the zone, as can be seen in the body
of <code>FractalInstance</code>.  The zone is also also a namespace for the instance name, so when
we list the instances behind each load balancer in the <code>google_compute_target_pool</code>
object, we compute the zone for each instance there as well.</p>

<p><code>FractalInstance</code> also specifies some default API access scopes and tags, as well as
the network over which the instances communicate.  It extends <code>GcpInstance</code> from
{{terraform_jsonnet|safe}}, which brings Default service account scopes, the network, and the
startup script to the top level, and provides some defaults for other parameters.</p>

<p>Back in {{service_jsonnet|safe}} we now have the instance definitions themselves.  Jsonnet can
help us define many instances that are essentially replicas.  The instances are actually given as a
map (i.e. an object), since the instances each have unique names in Terraform.  There are 3 different kinds of instances:
application server, db (cassandra) and tile generation.  The replicas in
each case are formed as an object mapping their names to definitions, and these 3 objects are
composed with the <code>+</code> operator.</p>

<p>The appserv and tilegen replicas are given using an object comprehension in which the field name
and value are computed with <code>k</code> set to each index in the given list.  The variable
<code>k</code> also ends up as an argument to <code>FractalInstance</code> and thus defines the
zone.  In both cases, we also place a file <tt>/var/www/conf.json</tt>.  This is read by the Python
code at startup and used to configure the service.  In the tilegen replicas the configuration comes
from <code>ApplicationConf</code> from the top of the file.  In the appserv instances, this
configuration is extended with some extra fields before being put in the file.  In both cases, the
startup script has added an extra line, computed by <code>self.addFile()</code>, a method (i.e. a
field containing a function) inherited from <code>GcpInstance</code> in {{terraform_jsonnet|safe}}.
Examining its definition shows that it uses echo to write the given content to the given filename.
</p>



<h2>Using The Configuration</h2>

<p>In order to reproduce this case study there are a few pre-requisites:</p>

<ul>

<li>A Linux or OSX system with GNU Make</li>

<li>Packer, built from github, in your $PATH.</li>

<li>Terraform, built with <a href="https://github.com/hashicorp/terraform/pull/588">PR #588</a>, in
your $PATH.</li>

<li>Jsonnet, built from github (this is also where you find the configuration and source code).</li>

<li>An account on Google Cloud Platform (hosting the application will incur charges).</li>

</ul>

<p>Once those are satisfied, follow these steps:</p>

<ol>

<li>In the Google Cloud Platform console, open your project, go to credentials, and create a new
service account.  This will automatic download a p12 key, which you can delete as we will not be
using it.  Instead, click the button to download a JSON key, move it to the fractal directory, and
call it <tt>service_account_key.json</tt>.</li>

<li>Create a credentials.jsonnet file based on the template, fill in your GCP project name and make
up some unique passwords.</li>

</ol>

<h3>Initial Deployment and Tear Down</h3>

<p>To deploy the app, run make -j.  This should start running 3 Packer builds in parallel.  In a
separate terminal, use tail -f *.log in order to watch their progress.  When the images are built,
Terraform is run and will show you the proposed changes.  Enter y to confirm the changes.  In time,
the application will be deployed.  Now you only need the appserv ip address to connect to it.  You
can get this using "gcloud compute addresses list" or by navigating the Google Cloud Platform
console to "networks".  Opening that ip in a web browser should take you to the application itself.
</p>

<p>The application can be brought down again with terraform destroy.  This will not destroy the
Packer images, they can be deleted from the console or from gcloud.</p>

<p>Many people rightly point out that a production web service is brought up once and then
maintained indefinitely afterwards, until it looks completely different to its original form.  It is
never torn down, as to do so would cause an outage and lose all the data in the databases.  However
there are two cases where bringing up a <i>copy</i> of a production service is useful:  Firstly, one
can do full integration testing on a throw-away deployment.  This can even be automated.  Secondly,
it can be used for load testing.</p>


<h3>Add / Remove Cassandra Nodes</h3>

<p>Managing the Cassandra cluster requires a combination of configuration alteration (to control the
fundamental compute resources) and use of the Cassandra commandline tool "nodetool".  For example
"nodetool status fractal" will give information about the cluster.</p>

<p>To add a new node, simply edit {{service_jsonnet|safe}}, add another instance in the Terraform
configuration and run make -j.  Confirm the changes (the only change should be the new instance,
e.g. db4).  It will start up and soon become part of the cluster.</p>

<p>To remove a node, first decommission it using nodetool -h HOSTNAME decommission.  When that is
complete, update {{service_jsonnet|safe}} to remove the resource and run make -j again.  Confirm the
removal of the instance.</p>

<p>If a node is lost (e.g. a disk error), or you removed it without first decommissioning it, the
cluster will expect it to return at some point (as if it were temporarily powered down or on the
wrong side of a network split).  This situationc an be rectified with nodetool removenode UUID, run
from any other node in the cluster.  It is probably also necessary to run nodetool repair on the
other nodes.</p>


<h3>Canary Change to Application Server</h3>

<h2>Conclusion</h2>

{% include 'footer.html.jinja' %}
